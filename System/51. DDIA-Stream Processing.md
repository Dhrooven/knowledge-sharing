# Stream Processing

In general, a “stream” refers to data that is **incrementally made available over time**. The concept appears in many 
places: in the stdin and stdout of Unix, programming languages (lazy lists), filesystem APIs (such as Java’s 
FileInputStream), TCP connections, delivering audio and video over the internet, and so on.

In this chapter we will look at **event streams** as a **data management** mechanism.

## 1. Transmitting Event Streams

A common approach for notifying consumers about new events is to use a messaging system: a producer sends a message 
containing the event, which is then **pushed** to consumers.

Within this publish/subscribe model, different systems take a wide range of approaches. To differentiate the systems,
it is particularly helpful to ask the following two questions:
- What happens if the producers send messages faster than the consumers can process them?
  - drop messages
  - buffer messages in a queue
  - apply backpressure (also known as flow control; i.e., blocking the producer from sending more messages)
- What happens if nodes crash or temporarily go offline—are any messages lost?
  - writing to disk
  - replication
  - afford to sometimes lose messages
  
### Message brokers
A widely used alternative is to send messages via a message broker (also known as a message queue), which is 
essentially a kind of database that is optimized for handling message streams. It runs as a server, with producers 
and consumers connecting to it as clients.

By centralizing the data in the broker, these systems can more easily tolerate clients that come and go (connect, 
disconnect, and crash), and the question of **durability** is moved to the broker instead.

A consequence of queueing is also that consumers are generally **asynchronous**: when a producer sends a message, it 
normally only waits for the broker to confirm that it has buffered the message and does not wait for the message to 
be processed by consumers.

#### AMQP/JMS-style message broker

The broker assigns individual messages to consumers, and consumers acknowledge individual messages when they have 
been successfully processed. Messages are deleted from the broker once they have been acknowledged.

RabbitMQ and Azure Service Bus are AMQP/JMS-style message brokers.

#### Log-based message broker

The broker assigns all messages in a partition to the same consumer node, and always delivers messages in the same 
order.

Parallelism is achieved through partitioning. Within each partition, the broker assigns a monotonically increasing sequence number, or **offset**, to every
**message**. There is **no ordering guarantee across different partitions**. Consumers track their progress by 
checkpointing the **offset** of the last message they have processed.

The broker retains messages on disk, so it is possible to jump back and reread old messages if necessary.

A **topic** can then be defined as **a group of partitions** that all carry messages of the same type, as shown below.

![image](https://user-images.githubusercontent.com/47337188/227669705-09cccfd4-05c8-4c88-860e-c04e4ddba8cd.png)

Apache Kafka and Amazon Kinesis Streams are log-based message brokers.

## 2. Databases and Streams

we saw that log-based message brokers have been successful in taking ideas from databases and applying them to 
messaging. We can also go in reverse: take **ideas from messaging and streams**, and apply them **to databases**.

We said previously that an **event** is a record of something that happened at some point in time. The thing that 
happened may be a user action (e.g., typing a search query), or a sensor reading, but it may also be **a write to a** 
**database**.

Representing databases as streams opens up powerful opportunities for **integrating systems**. You can keep derived 
data systems such as search indexes, caches, and analytics systems continually up to date by consuming the log of 
changes and applying them to the derived systems.

## 3. Processing Streams

There are 3 options to process streams:

- You can take the data in the events and **write** it to a database, cache, search index, or similar storage system, 
  from where it can then be **queried** by other clients.
- You can **push** the events **to users** in some way, for example by sending email alerts or push notifications, or by 
  streaming the events to a real-time dashboard where they are visualized. In this case, a human is the ultimate 
  consumer of the stream.
- You can process one or more input streams to **produce** one or more output **streams**. Streams may go through a pipeline 
  consisting of several such processing stages before they **eventually end up at an output** (option 1 or 2). A 
  piece of code that processes streams like this is known as an **operator** or a **job**.

### Reasoning About Time

Stream processors often need to deal with **time**. 

Many stream processing frameworks use the **local system clock** on the processing machine (**the processing time**) to 
determine windowing.

A tricky problem when defining windows in terms of **event time** is that you can never be sure when you have received 
all of the events for a particular window, or whether there are some events still to come.

Assigning timestamps to events is even more difficult when events can be buffered at several points in the system.

To adjust for incorrect device clocks, one approach is to log 3 timestamps:

1. The time at which the event **occurred**, according to the device clock
2. The time at which the event was **sent** to the server, according to the device clock
3. The time at which the event was **received** by the server, according to the server clock

By subtracting the second timestamp from the third, you can estimate the offset between the **device clock** and the 
**server clock** (assuming the network delay is negligible compared to the required timestamp accuracy). You can then 
apply that offset to the event timestamp, and thus estimate the **true time at which the event actually occurred**.

[To be continued]